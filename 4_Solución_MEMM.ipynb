{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCL0r4ap2H4T"
   },
   "source": [
    "## Construcción de un modelo markoviano de máxima entropía\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7FaYXYmy5ahC",
    "outputId": "d5248762-8e5b-4a45-9006-c9cbec9e61e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting conllu\n",
      "  Downloading https://files.pythonhosted.org/packages/25/1b/b1481ba63198eb7b88d715945682bb7fc986ec5dda2d26e313ecdea8a5f6/conllu-4.2.2-py2.py3-none-any.whl\n",
      "Installing collected packages: conllu\n",
      "Successfully installed conllu-4.2.2\n",
      "Collecting stanza\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/e7/8b/3a9e7a8d8cb14ad6afffc3983b7a7322a3a24d94ebc978a70746fcffc085/stanza-1.1.1-py3-none-any.whl (227kB)\n",
      "\u001B[K     |████████████████████████████████| 235kB 5.1MB/s \n",
      "\u001B[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanza) (1.19.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanza) (4.41.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanza) (2.23.0)\n",
      "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from stanza) (1.7.0+cu101)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanza) (3.12.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2.10)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.8)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (3.7.4.3)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (51.0.0)\n",
      "Installing collected packages: stanza\n",
      "Successfully installed stanza-1.1.1\n",
      "Cloning into 'UD_Spanish-AnCora'...\n",
      "remote: Enumerating objects: 31, done.\u001B[K\n",
      "remote: Counting objects: 100% (31/31), done.\u001B[K\n",
      "remote: Compressing objects: 100% (22/22), done.\u001B[K\n",
      "remote: Total 526 (delta 14), reused 25 (delta 9), pack-reused 495\u001B[K\n",
      "Receiving objects: 100% (526/526), 115.95 MiB | 9.93 MiB/s, done.\n",
      "Resolving deltas: 100% (361/361), done.\n"
     ]
    }
   ],
   "source": [
    "!pip install conllu\n",
    "!pip install stanza\n",
    "!git clone https://github.com/UniversalDependencies/UD_Spanish-AnCora.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Oi9R1bn2Qmg"
   },
   "source": [
    "### Entrenamiento del modelo - cálculo de conteos\n",
    "\n",
    "Para este modelo consideramos el cálculo de las probabilidades: \n",
    "\n",
    "$$P(t_i | w_i, t_{i-1}) =\\frac{C(w_i, t_i, t_{i-1})}{C(w_i, t_{i-1})} $$\n",
    "\n",
    "* `uniqueFeatureDict` $C(tag|word,prevtag) = C(w_i, t_i, t_{i-1})$\n",
    "* `contextDict` $C(word,prevtag) = C(w_i, t_{i-1})$\n",
    "\n",
    "En este caso cuando consideremos el primer elemento de una frase $w_0$, no existirá un elemento anterior $w_{-1}$ y por lo tanto, tampoco una etiqueta previa $t_{-1}$, podemos modelar este problema asumiendo que existe una etiqueta \"None\", para estos casos: \n",
    "\n",
    "$$P(t_0|w_0,t_{-1}) = P(t_0|w_0,\\text{\"None\"})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "0riVspXk1nHl"
   },
   "outputs": [],
   "source": [
    "from conllu import parse_incr\n",
    "\n",
    "wordcorpus = 'form'\n",
    "tagtype = 'upos'\n",
    "data_file = open(\"UD_Spanish-AnCora/es_ancora-ud-dev.conllu\", \"r\", encoding=\"utf-8\")\n",
    "\n",
    "uniqueFeatureDict = {}\n",
    "contextDict = {}\n",
    "\n",
    "# Calculando conteos (pre-probabilidades)\n",
    "for tokenlist in parse_incr(data_file):\n",
    "  prevtag = \"None\"\n",
    "  for token in tokenlist:\n",
    "    tag = token[tagtype]\n",
    "    word = token[wordcorpus].lower()\n",
    "\n",
    "    #C(tag,word,prevtag)\n",
    "    c1 = tag + '(,)' + word + '(,)' + prevtag\n",
    "    if c1 in uniqueFeatureDict.keys():\n",
    "      uniqueFeatureDict[c1] += 1\n",
    "    else:\n",
    "      uniqueFeatureDict[c1] = 1\n",
    "\n",
    "    #C(word|prevtag)  \n",
    "    c2 = word + '(,)' + prevtag\n",
    "    if c2 in contextDict.keys():\n",
    "      contextDict[c2] += 1\n",
    "    else:\n",
    "      contextDict[c2] = 1\n",
    "    prevtag=tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6npPBeu_8OzK"
   },
   "source": [
    "### Entrenamiento del modelo - cálculo de probabilidades\n",
    "\n",
    "$$P(t_i|w_i, t_{i-1}) = \\frac{C(t_i, w_i, t_{i-1})}{C(w_i, t_{i-1})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "ywGEbMTw73NU"
   },
   "outputs": [],
   "source": [
    "posteriorProbDict = {}\n",
    "\n",
    "for key in uniqueFeatureDict.keys():\n",
    "  prob = key.split('(,)')\n",
    "  posteriorProbDict[prob[0] + '(|)' + prob[1] + '(,)' + prob[2]] = uniqueFeatureDict[key]/contextDict[prob[1] + '(,)' + prob[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eA9wqVQPBiVN",
    "outputId": "62b50f0f-e7ae-4e9f-85d4-9c01620dfb20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La combinación 'cuando(,)CCONJ' de 3 solo suma: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# Aquí verificamos que todas las probabilidades \n",
    "# por cada contexto 'word,prevtag' suman 1.0\n",
    " \n",
    "for base_context in contextDict.keys():\n",
    "  sumprob = 0\n",
    "  items = 0\n",
    "  \n",
    "  for key in posteriorProbDict.keys():\n",
    "      if key.split('(|)')[1] == base_context:\n",
    "        sumprob += posteriorProbDict[key]\n",
    "        items += 1\n",
    "  if sumprob != 1:\n",
    "    print(\"La combinación '\" + base_context + \"'\", \"de \" + str(items), \"solo suma: \" + str(sumprob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leSuajlOA-3c"
   },
   "source": [
    "### Distribución inicial de estados latentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nU4ShPmDQRp0",
    "outputId": "34a5ff3a-199a-4324-9bf5-053f2b1dd56a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADJ',\n",
       " 'ADP',\n",
       " 'ADV',\n",
       " 'AUX',\n",
       " 'CCONJ',\n",
       " 'DET',\n",
       " 'INTJ',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " 'PART',\n",
       " 'PRON',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'SCONJ',\n",
       " 'SYM',\n",
       " 'VERB',\n",
       " '_'}"
      ]
     },
     "execution_count": 90,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identificamos las categorias gramaticales 'upos' unicas en el corpus\n",
    "stateSet = set([k.split('(,)')[1] for k in contextDict.keys()])\n",
    "stateSet.remove(\"None\")\n",
    "stateSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v51B_Xq39iV4",
    "outputId": "09f1c1ff-92f9-4a63-ad76-228784a7bd44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADJ': 0,\n",
       " 'ADP': 1,\n",
       " 'ADV': 2,\n",
       " 'AUX': 3,\n",
       " 'CCONJ': 4,\n",
       " 'DET': 5,\n",
       " 'INTJ': 6,\n",
       " 'NOUN': 7,\n",
       " 'NUM': 8,\n",
       " 'PART': 9,\n",
       " 'PRON': 10,\n",
       " 'PROPN': 11,\n",
       " 'PUNCT': 12,\n",
       " 'SCONJ': 13,\n",
       " 'SYM': 14,\n",
       " 'VERB': 15,\n",
       " '_': 16}"
      ]
     },
     "execution_count": 91,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enumeramos las categorias con numeros para asignar a \n",
    "# las columnas de la matriz de Viterbi\n",
    "tagStateDict = {}\n",
    "for i, state in enumerate(sorted(stateSet)):\n",
    "    tagStateDict[state] = i\n",
    "tagStateDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oACerPVe_Awz",
    "outputId": "5a59b6d7-741b-486a-aa24-078b965ed8f0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADJ': 0.010136315973435861,\n",
       " 'ADP': 0.1574274729115694,\n",
       " 'ADV': 0.07577770010485844,\n",
       " 'AUX': 0.022789234533379936,\n",
       " 'CCONJ': 0.036980076896190144,\n",
       " 'DET': 0.34799021321216356,\n",
       " 'INTJ': 0.0020272631946871723,\n",
       " 'NOUN': 0.025026214610276126,\n",
       " 'NUM': 0.0068507514854945824,\n",
       " 'PART': 0.002446696959105208,\n",
       " 'PRON': 0.04173365955959455,\n",
       " 'PROPN': 0.10506815798671792,\n",
       " 'PUNCT': 0.09143656064313177,\n",
       " 'SCONJ': 0.027123383432366307,\n",
       " 'SYM': 0.0004893393918210416,\n",
       " 'VERB': 0.04557846906675987,\n",
       " '_': 0.0011184900384480952}"
      ]
     },
     "execution_count": 92,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = open(\"UD_Spanish-AnCora/es_ancora-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
    "\n",
    "initTagStateProb = {} # \\rho_i^{(0)}\n",
    "count = 0 # cuenta la longitud del corpus\n",
    "for tokenlist in parse_incr(data_file):\n",
    "  count += 1\n",
    "  tag = tokenlist[0]['upos']\n",
    "  if tag in initTagStateProb.keys():\n",
    "    initTagStateProb[tag] += 1\n",
    "  else:\n",
    "    initTagStateProb[tag] = 1\n",
    "\n",
    "for key in initTagStateProb.keys():\n",
    "  initTagStateProb[key] /= count\n",
    "\n",
    "initTagStateProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cHz3d0HL1gef",
    "outputId": "b34c380d-c5a4-47eb-a00d-8b1fa4506855",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999998"
      ]
     },
     "execution_count": 93,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(initTagStateProb.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neEbST9IDq8f",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Construcción del algoritmo de Viterbi\n",
    "\n",
    "Dada una secuencia de palabras $\\{p_1, p_2, \\dots, p_n \\}$, y un conjunto de categorias gramaticales dadas por la convención `upos`, se considera la matriz de probabilidades de Viterbi así:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c c}\n",
    "\\begin{array}{c c c c}\n",
    "\\text{ADJ} \\\\\n",
    "\\text{ADV}\\\\\n",
    "\\text{PRON} \\\\\n",
    "\\vdots \\\\\n",
    "{}\n",
    "\\end{array} \n",
    "&\n",
    "\\left[\n",
    "\\begin{array}{c c c c}\n",
    "\\nu_1(\\text{ADJ}) & \\nu_2(\\text{ADJ}) & \\dots  & \\nu_n(\\text{ADJ})\\\\\n",
    "\\nu_1(\\text{ADV}) & \\nu_2(\\text{ADV}) & \\dots  & \\nu_n(\\text{ADV})\\\\ \n",
    "\\nu_1(\\text{PRON}) & \\nu_2(\\text{PRON}) & \\dots  & \\nu_n(\\text{PRON})\\\\\n",
    "\\vdots & \\vdots & \\dots & \\vdots \\\\ \\hdashline\n",
    "p_1 & p_2 & \\dots & p_n \n",
    "\\end{array}\n",
    "\\right] \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Donde las probabilidades de Viterbi en la primera columna (para una categoria $i$) están dadas por: \n",
    "\n",
    "$$\n",
    "\\nu_1(i) = \\underbrace{\\rho_i^{(0)}}_{\\text{probabilidad inicial}} \\times P(i \\vert p_1, \\text{\"None\"})\n",
    "$$\n",
    "\n",
    "y para las siguientes columnas: \n",
    "\n",
    "$$\n",
    "\\nu_{t}(j) = \\max_i \\{ \\overbrace{\\nu_{t-1}(i)}^{\\text{estado anterior}} \\times P(j \\vert p_t, i) \\}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o7fDedW5BE-q",
    "outputId": "5cf33040-3d85-407c-8c59-3156a95d8559"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 27.6MB/s]                    \n",
      "2021-01-03 10:58:18 INFO: Downloading default packages for language: es (Spanish)...\n",
      "2021-01-03 10:58:21 INFO: File exists: /root/stanza_resources/es/default.zip.\n",
      "2021-01-03 10:58:29 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
      "2021-01-03 10:58:29 INFO: Loading these models for language: es (Spanish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "=======================\n",
      "\n",
      "2021-01-03 10:58:29 INFO: Use device: cpu\n",
      "2021-01-03 10:58:29 INFO: Loading: tokenize\n",
      "2021-01-03 10:58:29 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import stanza\n",
    "stanza.download('es')\n",
    "nlp = stanza.Pipeline('es', processors='tokenize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X8yBXTq-PHGY",
    "outputId": "97e3fcd6-85f1-4aad-b9a9-97c9f0bd93b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.31635474, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.34799021, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.31635474, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.03163547, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 96,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ViterbiMatrix(\n",
    "    secuencia, \n",
    "    posteriorProbDict = posteriorProbDict, \n",
    "    initTagStateProb = initTagStateProb):\n",
    "  \n",
    "  doc = nlp(secuencia)\n",
    "  if len(doc.sentences) > 1:\n",
    "    raise ValueError('secuencia must be a string!')\n",
    "  seq = [word.text for word in doc.sentences[0].words]\n",
    "  viterbiProb = np.zeros((17, len(seq)))\n",
    "  \n",
    "  # inicialización primera columna\n",
    "  for tag in tagStateDict.keys():\n",
    "    tag_row = tagStateDict[tag]\n",
    "    key = tag + '(|)' + seq[0].lower() + '(,)' + \"None\"\n",
    "    try:\n",
    "      viterbiProb[tag_row, 0] = initTagStateProb[tag] * posteriorProbDict[key]\n",
    "    except: \n",
    "      pass\n",
    "  \n",
    "  # computo de las siguientes columnas\n",
    "  for col in range(1, len(seq)):\n",
    "    for tag in tagStateDict.keys():\n",
    "      tag_row = tagStateDict[tag]\n",
    "      possible_probs = []\n",
    "      for prevtag in tagStateDict.keys(): \n",
    "        prevtag_row = tagStateDict[prevtag]\n",
    "        key = tag + '(|)' + seq[col].lower() + '(,)' + prevtag\n",
    "        try:\n",
    "          possible_probs.append(\n",
    "              viterbiProb[prevtag_row, col-1] * posteriorProbDict[key])\n",
    "        except:\n",
    "          possible_probs.append(0)\n",
    "      viterbiProb[tag_row, col] = max(possible_probs)\n",
    "\n",
    "  return viterbiProb\n",
    "\n",
    "ViterbiMatrix('el mundo es pequeño')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XKC3bh60X3h3",
    "outputId": "7379af22-def5-46b8-ccda-740bc116c3d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('el', 'DET'), ('mundo', 'NOUN'), ('es', 'AUX'), ('pequeño', 'ADJ')]"
      ]
     },
     "execution_count": 101,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ViterbiTags(\n",
    "    secuencia, \n",
    "    posteriorProbDict = posteriorProbDict, \n",
    "    initTagStateProb = initTagStateProb):\n",
    "  \n",
    "  doc = nlp(secuencia)\n",
    "  if len(doc.sentences) > 1:\n",
    "    raise ValueError('secuencia must be a string!')\n",
    "  seq = [word.text for word in doc.sentences[0].words]\n",
    "  viterbiProb = np.zeros((17, len(seq)))\n",
    "  \n",
    "  # inicialización primera columna\n",
    "  for tag in tagStateDict.keys():\n",
    "    tag_row = tagStateDict[tag]\n",
    "    key = tag + '(|)' + seq[0].lower() + '(,)' + \"None\"\n",
    "    try:\n",
    "      viterbiProb[tag_row, 0] = initTagStateProb[tag] * posteriorProbDict[key]\n",
    "    except: \n",
    "      pass\n",
    "  \n",
    "  # computo de las siguientes columnas\n",
    "  for col in range(1, len(seq)):\n",
    "    for tag in tagStateDict.keys():\n",
    "      tag_row = tagStateDict[tag]\n",
    "      possible_probs = []\n",
    "      for prevtag in tagStateDict.keys(): \n",
    "        prevtag_row = tagStateDict[prevtag]\n",
    "        key = tag + '(|)' + seq[col].lower() + '(,)' + prevtag\n",
    "        try:\n",
    "          possible_probs.append(\n",
    "              viterbiProb[prevtag_row, col-1] * posteriorProbDict[key])\n",
    "        except:\n",
    "          possible_probs.append(0)\n",
    "      viterbiProb[tag_row, col] = max(possible_probs)\n",
    "\n",
    "  # contruccion de secuencia de tags\n",
    "  etiquetas = []\n",
    "  for i, p in enumerate(seq):\n",
    "    for tag in tagStateDict.keys():\n",
    "      if tagStateDict[tag] == np.argmax(viterbiProb[:, i]):\n",
    "        etiquetas.append((p, tag))\n",
    "\n",
    "\n",
    "  return etiquetas\n",
    "\n",
    "ViterbiTags('el mundo es pequeño')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99RZyAyl9iV5",
    "outputId": "369557da-6325-4f4c-842e-b02774a87b27",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('estos', 'DET'),\n",
       " ('instrumentos', 'NOUN'),\n",
       " ('han', 'AUX'),\n",
       " ('de', 'ADP'),\n",
       " ('rasgar', 'VERB')]"
      ]
     },
     "execution_count": 102,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ViterbiTags('estos instrumentos han de rasgar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿ Siguientes Pasos ? \n",
    "\n",
    "El modelo construido, aunque es la base de un MEMM, no explota todo el potencial del concepto  que estos modelos representan, en nuestro caso sencillo consideramos solo un **feature** para predecir la categoría gramatical: $<w_i, t_{i-1}>$. Es decir, las probabilidades de una cierta etiqueta $t_i$ dada una observación $<w_i, t_{i-1}>$ se calculan contando eventos donde se observe que $<w_i, t_{i-1}>$ sucede simultáneamente con $t_i$. \n",
    "\n",
    "La generalización de esto (donde puedo considerar multiples observaciones o **features**, y a partir de estos inferir la categoría gramatical) se hace construyendo las llamadas **feature-functions**, donde estas funciones toman valores de 0 o 1, cuando se cumplan las condiciones de la observación o feature en cuestion. En general podemos considerar una **feature-function** como : \n",
    "\n",
    "$$f_a(t, o) = f_a(\\text{tag}, \\text{observation}) = \n",
    "\\begin{cases}\n",
    "  1 , & \\text{se cumple condición } a \\\\\n",
    "  0, & \\text{en caso contrario}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "donde la condición $a$ es una relacion entre los valores que tome $\\text{tag}$ y $\\text{context}$, por ejemplo:\n",
    "\n",
    "$$f_a(t, o) = f_a(\\text{tag}, \\text{observation}) = \n",
    "\\begin{cases}\n",
    "  1 , & (t_i, t_{i-1}) = \\text{('VERB', 'ADJ')} \\\\\n",
    "  0, & \\text{en caso contrario}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Al considerar varias funciones, y por lo tanto varios features observables, consideramos una combinacion lineal de estos por medio de un coeficiente que multiplique a cada función: \n",
    "\n",
    "$$\n",
    "\\theta_1 f_1(t, o) + \\theta_2 f_2(t, o) + \\dots\n",
    "$$\n",
    "\n",
    "donde los coeficientes indicarán cuales features son más relevantes y por lo tanto pesan más para la decisión del resultado del modelo. De esta manera los coeficientes $\\theta_j$ se vuelven parámetros del modelo que deben ser optimizados (esto puede realizarse con cualquier técnica de optimizacion como el Gradiente Descendente). Ahora, las probabilidades que pueden obtener usando un softmax sobre estas combinaciones lineales de features: \n",
    "\n",
    "$$\n",
    "P = \\prod_i \\frac{\\exp{\\left(\\sum_j \\theta_j f_j(t_i, o)\\right)}}{\\sum_{t'}\\exp{\\left(\\sum_j \\theta_j f_j(t', o)\\right)}}\n",
    "$$\n",
    "\n",
    "Así, lo que buscamos con el algoritmo de optimización es encontrar los parámetros $\\theta_j$ que maximizan la probabilidad anterior. En NLTK encontramos la implementación completa de un clasificador de máxima entropia que no esta restringido a relaciones markovianas: https://www.nltk.org/_modules/nltk/classify/maxent.html\n",
    "\n",
    "Un segmento resumido de la clase en python que implementa este clasificador en NLTK lo encuentras así: \n",
    "\n",
    "```\n",
    "class MaxentClassifier(ClassifierI):\n",
    "\n",
    "    def __init__(self, encoding, weights, logarithmic=True):\n",
    "        self._encoding = encoding\n",
    "        self._weights = weights\n",
    "        self._logarithmic = logarithmic\n",
    "        assert encoding.length() == len(weights)\n",
    "\n",
    "    def labels(self):\n",
    "        return self._encoding.labels()\n",
    "\n",
    "    def set_weights(self, new_weights):\n",
    "        self._weights = new_weights\n",
    "        assert self._encoding.length() == len(new_weights)\n",
    "\n",
    "\n",
    "    def weights(self):\n",
    "        return self._weights\n",
    "\n",
    "    def classify(self, featureset):\n",
    "        return self.prob_classify(featureset).max()\n",
    "\n",
    "    def prob_classify(self, featureset):\n",
    "        ### ...\n",
    "\n",
    "        # Normalize the dictionary to give a probability distribution\n",
    "        return DictionaryProbDist(prob_dict, log=self._logarithmic, normalize=True)\n",
    "\n",
    "    @classmethod\n",
    "    def train(\n",
    "        cls,\n",
    "        train_toks,\n",
    "        algorithm=None,\n",
    "        trace=3,\n",
    "        encoding=None,\n",
    "        labels=None,\n",
    "        gaussian_prior_sigma=0,\n",
    "        **cutoffs\n",
    "    ):\n",
    "     ### ......\n",
    "```\n",
    "\n",
    "Donde te das cuenta de la forma que tienen las clases en NLTK que implementan clasificadores generales. Aquí vemos que la clase `MaxentClassifier` es una subclase de una más general `ClassifierI` la cual representa el proceso de clasificación general de categoría única (es decir, que a cada data-point le corresponda solo una categoria), también que esta clase depende de definir un `encoding`\n",
    " y unos pesos `weights` : \n",
    "\n",
    "```\n",
    "class MaxentClassifier(ClassifierI):\n",
    "\n",
    "    def __init__(self, encoding, weights, logarithmic=True):\n",
    "```\n",
    "\n",
    "los pesos corresponden a los parámetros $\\theta_i$. Y el encoding es el que corresponde a las funciones $f_a(t, o)$ que dan como resultado valores binarios $1$ o $0$.\n",
    "\n",
    "La documentación de NLTK te puede dar mas detalles de esta implementación: https://www.nltk.org/api/nltk.classify.html\n",
    "\n",
    "Finalmente, un ejemplo completo de uso y mejora de un modelo de máxima entropía, se puede encontrar en este fork para tenerlo como referencia y poder jugar y aprender con él: \n",
    "\n",
    "https://github.com/pachocamacho1990/nltk-maxent-pos-tagger\n",
    "\n",
    "El cual fue desarrollado originalmente por Arne Neumann (https://github.com/arne-cl) basado en los fueatures propuestos por Ratnaparki en 1996 para la tarea de etiquetado por categorias gramaticales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozD_EqlbaMRB"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mU5SDxi3F819"
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to=python 4_Solución_MEMM.ipynb"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "4_Solución_MEMM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}