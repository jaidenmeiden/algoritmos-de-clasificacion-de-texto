{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "858EOFNBPxDs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: conllu in ./venv/lib/python3.7/site-packages (4.2.2)\n",
      "fatal: la ruta de destino 'UD_Spanish-AnCora' ya existe y no es un directorio vacío.\n"
     ]
    }
   ],
   "source": [
    "# instalacion de dependencias previas\n",
    "!pip install conllu\n",
    "!git clone https://github.com/UniversalDependencies/UD_Spanish-AnCora.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAHDLyQQcboh"
   },
   "source": [
    "# Carga del modelo HMM previamente entrenado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAHDLyQQcboh"
   },
   "source": [
    "Cargamos las probabilidades del modelo HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "9jwZedC6RVJy"
   },
   "outputs": [],
   "source": [
    "transitionProbdict = np.load('transitionHMM.npy', allow_pickle='TRUE').item()\n",
    "emissionProbdict = np.load('emissionHMM.npy', allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAHDLyQQcboh"
   },
   "source": [
    "Identificamos las categorias gramaticales 'upos' unicas en el corpus.\n",
    "\n",
    "Obtenemos las llaves de la colección de probabilidades de emisión con `emissionProbdict.keys()` y creamos un bucle recorriendo la lista de llaves `[k for k in emissionProbdict.keys()` y de cada llave obtenida captuarmos unicamente la categoría gramatical `k.split('|')[1]` en la segunda posición de la llave. Para que no nos muestre categorías repetidas aplicamos la función `set()`, donde nos debe mostrar **17 registros** según la convención internacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "Ezi9gcquUw55",
    "outputId": "0b03240b-6688-456b-ecf0-098e1ff6e1a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADJ',\n",
       " 'ADP',\n",
       " 'ADV',\n",
       " 'AUX',\n",
       " 'CCONJ',\n",
       " 'DET',\n",
       " 'INTJ',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " 'PART',\n",
       " 'PRON',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'SCONJ',\n",
       " 'SYM',\n",
       " 'VERB',\n",
       " '_'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stateSet = set([k.split('|')[1] for k in emissionProbdict.keys()])\n",
    "stateSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAHDLyQQcboh"
   },
   "source": [
    "Enumeramos las categorias con números para asignar a las columnas (Asignamos un número entero) de la matriz de Viterbi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "-aVD0jboWKGh",
    "outputId": "00d32831-8964-4ad6-de81-ffe5ba2d3a46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADJ': 0,\n",
       " 'ADP': 1,\n",
       " 'ADV': 2,\n",
       " 'AUX': 3,\n",
       " 'CCONJ': 4,\n",
       " 'DET': 5,\n",
       " 'INTJ': 6,\n",
       " 'NOUN': 7,\n",
       " 'NUM': 8,\n",
       " 'PART': 9,\n",
       " 'PRON': 10,\n",
       " 'PROPN': 11,\n",
       " 'PUNCT': 12,\n",
       " 'SCONJ': 13,\n",
       " 'SYM': 14,\n",
       " 'VERB': 15,\n",
       " '_': 16}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagStateDict = {}\n",
    "for i, state in enumerate(sorted(stateSet)):\n",
    "    tagStateDict[state] = i\n",
    "tagStateDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SNuWx-ScSTg"
   },
   "source": [
    "# Distribucion inicial de estados latentes\n",
    "\n",
    "Calculamos distribución inicial de estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "put9Dyk1Yl2A",
    "outputId": "102c4102-3a17-48dd-93b2-40c95a96ff04"
   },
   "outputs": [],
   "source": [
    "from conllu import parse_incr \n",
    "wordList = []\n",
    "data_file = open(\"UD_Spanish-AnCora/es_ancora-ud-dev.conllu\", \"r\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SNuWx-ScSTg"
   },
   "source": [
    "En `initTagStateProb` (Guarda los `\\rho_i^{(0)}`, que son los **rhos** del estado **i** en el momento **0**) es donde guardamos la probabilidad de que encuentre una categoría gramatical al principio de una frase en el corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DET': 0.3633615477629988,\n",
       " 'PROPN': 0.1124546553808948,\n",
       " 'ADP': 0.16384522370012092,\n",
       " 'PRON': 0.034461910519951636,\n",
       " 'SCONJ': 0.02418379685610641,\n",
       " 'ADV': 0.06287787182587666,\n",
       " 'PUNCT': 0.07799274486094317,\n",
       " 'VERB': 0.04353083434099154,\n",
       " 'ADJ': 0.010882708585247884,\n",
       " 'CCONJ': 0.03325272067714631,\n",
       " 'NOUN': 0.02720677146311971,\n",
       " '_': 0.0006045949214026602,\n",
       " 'INTJ': 0.0006045949214026602,\n",
       " 'AUX': 0.022370012091898428,\n",
       " 'NUM': 0.01995163240628779,\n",
       " 'SYM': 0.0006045949214026602,\n",
       " 'PART': 0.0018137847642079807}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initTagStateProb = {} # \\rho_i^{(0)}\n",
    "count = 0 # cuenta la longitud del corpus\n",
    "for tokenlist in parse_incr(data_file):\n",
    "    count += 1\n",
    "    tag = tokenlist[0]['upos']\n",
    "    if tag in initTagStateProb.keys():\n",
    "        initTagStateProb[tag] += 1\n",
    "    else:\n",
    "        initTagStateProb[tag] = 1\n",
    "\n",
    "for key in initTagStateProb.keys():\n",
    "    initTagStateProb[key] /= count\n",
    "\n",
    "initTagStateProb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAHDLyQQcboh"
   },
   "source": [
    "Verificamos que la suma de las probabilidades es 1 (100%)\n",
    "\n",
    "En la forma **NO** elegante, sumamos los valores de la colección creando un blucle con el que recorremos la colleción por sus llaves creando una lista con todas las probabilidades, la cual convertimos a un arreglo de `numpy` para aplicar la función `sum()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "id": "J5Rltqj6bbcV",
    "outputId": "2477fc48-e9d1-4548-baf2-e6e218f36b99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([initTagStateProb[k] for k in initTagStateProb.keys()]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAHDLyQQcboh"
   },
   "source": [
    "En una forma más simple y eficiente, sumamos los valores de la colección accediendo directemante a la lista de valores de la colección con `list(initTagStateProb.values())`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(initTagStateProb.values())).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAHDLyQQcboh"
   },
   "source": [
    "En la forma muy eficiente, simplemente sumamos la lista de valores, sin utilizar `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(initTagStateProb.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjnAshwzxrKZ"
   },
   "source": [
    "# Construcción del algoritmo de Viterbi\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zX-_MnPexnm0"
   },
   "source": [
    "Dada una secuencia de palabras $\\{p_1, p_2, \\dots, p_n \\}$, y un conjunto de categorias gramaticales dadas por la convención `upos`, se considera la matriz de probabilidades de Viterbi así:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c c}\n",
    "\\begin{array}{c c c c}\n",
    "\\text{ADJ} \\\\\n",
    "\\text{ADV}\\\\\n",
    "\\text{PRON} \\\\\n",
    "\\vdots \\\\\n",
    "{}\n",
    "\\end{array} \n",
    "&\n",
    "\\left[\n",
    "\\begin{array}{c c c c}\n",
    "\\nu_1(\\text{ADJ}) & \\nu_2(\\text{ADJ}) & \\dots  & \\nu_n(\\text{ADJ})\\\\\n",
    "\\nu_1(\\text{ADV}) & \\nu_2(\\text{ADV}) & \\dots  & \\nu_n(\\text{ADV})\\\\ \n",
    "\\nu_1(\\text{PRON}) & \\nu_2(\\text{PRON}) & \\dots  & \\nu_n(\\text{PRON})\\\\\n",
    "\\vdots & \\vdots & \\dots & \\vdots \\\\ \\hdashline\n",
    "p_1 & p_2 & \\dots & p_n \n",
    "\\end{array}\n",
    "\\right] \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Donde las probabilidades de la primera columna (para una categoria $i$) están dadas por: \n",
    "\n",
    "$$\n",
    "\\nu_1(i) = \\underbrace{\\rho_i^{(0)}}_{\\text{probabilidad inicial}} \\times \\underbrace{P(p_1 \\vert i)}_{\\text{emisión}}\n",
    "$$\n",
    "\n",
    "luego, para la segunda columna (dada una categoria $j$) serán: \n",
    "\n",
    "$$\n",
    "\\nu_2(j) = \\max_i \\{ \\nu_1(i) \\times \\underbrace{P(j \\vert i)}_{\\text{transición}} \\times \\underbrace{P(p_2 \\vert j)}_{\\text{emisión}} \\}\n",
    "$$\n",
    "\n",
    "así, en general las probabilidades para la columna $t$ estarán dadas por: \n",
    "\n",
    "$$\n",
    "\\nu_{t}(j) = \\max_i \\{ \\overbrace{\\nu_{t-1}(i)}^{\\text{estado anterior}} \\times \\underbrace{P(j \\vert i)}_{\\text{transición}} \\times \\underbrace{P(p_t \\vert j)}_{\\text{emisión}} \\}\n",
    "$$\n",
    "\n",
    "### Debemos importar la librería NLTK, ya que debemos tokenizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "id": "rAyO788xPKra",
    "outputId": "a470923e-d446-4063-97a7-30e3e962cb18"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/oem/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # cargamos el paquete 'punkt' de NLTK\n",
    "from nltk import word_tokenize # importamos el tokenizador de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAHDLyQQcboh"
   },
   "source": [
    "Construimos la función `ViterbiMatrix` a la cual le pasamos la secuencia de palabras (Este `string` lo tenemos que tokenizar), la matriz de transición `A`, las probabilidades de emisión `B`, el diccionario de categorias con números para asignar a las columnas `tagStateDict` y la probabilidad de que encuentre una categoría gramatical al principio de una frase en el corpus `initTagStateProb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "9sJhQ35m5ASB",
    "outputId": "7cc43024-fe2c-4e43-a4d1-95952c3195de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('el', 'DET'), ('mundo', 'NOUN'), ('es', 'AUX'), ('pequeño', 'ADJ')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (secuencia, A, B, tagStateDict, initTagStateProb)\n",
    "# Eacribimos la función con valores predeterminados, pero podrian enviarse otras matrices\n",
    "def ViterbiTags(secuencia, \n",
    "                transitionProbdict=transitionProbdict, \n",
    "                emissionProbdict=emissionProbdict, \n",
    "                tagStateDict=tagStateDict, \n",
    "                initTagStateProb=initTagStateProb):\n",
    "    \n",
    "    # Tokenizamos la secuencia     \n",
    "    seq = word_tokenize(secuencia)\n",
    "    # Inicializamos la matrix de Viterbi, la cual inicia en cero\n",
    "    viterbiProb = np.zeros((17, len(seq)))  # upos tiene 17 categorias\n",
    "    \n",
    "    # inicialización primera columna\n",
    "    for tag in tagStateDict.keys():\n",
    "        tag_row = tagStateDict[tag]\n",
    "        word_tag = seq[0].lower() + '|' + tag\n",
    "        if word_tag in emissionProbdict.keys():\n",
    "            viterbiProb[tag_row, 0] = initTagStateProb[tag] * emissionProbdict[word_tag]\n",
    "            \n",
    "    for col in range(1, len(seq)):\n",
    "        for tag_actual in tagStateDict.keys():\n",
    "            tag_row = tagStateDict[tag_actual]\n",
    "            word_tag = seq[col].lower() + '|' + tag_actual\n",
    "            if word_tag in emissionProbdict.keys():\n",
    "                possible_probs = []\n",
    "                for tag_prev in tagStateDict.keys():\n",
    "                    tag_prev_row = tagStateDict[tag_prev]\n",
    "                    tag_prev_tag = tag_actual + '|' + tag_prev\n",
    "                    if tag_prev_tag in transitionProbdict.keys():\n",
    "                        if viterbiProb[tag_prev_row, col-1] > 0:\n",
    "                            possible_probs.append(\n",
    "                                viterbiProb[tag_prev_row, col-1] * transitionProbdict[tag_prev_tag] * emissionProbdict[word_tag])\n",
    "                viterbiProb[tag_row, col] = max(possible_probs)\n",
    "    \n",
    "    # contruccion de secuencia de tags\n",
    "    etiquetas = []\n",
    "    for i, p in enumerate(seq):\n",
    "        for tag in tagStateDict.keys():\n",
    "            # Buscamos en que fila esta la máxima probabilidad de todas las posibles filas\n",
    "            if tagStateDict[tag] == np.argmax(viterbiProb[:, i]):\n",
    "                #print(tagStateDict[tag], np.argmax(viterbiProb[:, i]))\n",
    "                etiquetas.append((p, tag))\n",
    "                \n",
    "    return etiquetas\n",
    "\n",
    "ViterbiTags('el mundo es pequeño')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98
    },
    "id": "J9CYMtNpuoKq",
    "outputId": "92aa7ea5-4aec-424f-efd9-1837d02bdf00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('estos', 'DET'),\n",
       " ('instrumentos', 'NOUN'),\n",
       " ('han', 'AUX'),\n",
       " ('de', 'ADP'),\n",
       " ('rasgar', 'VERB')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ViterbiTags('estos instrumentos han de rasgar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnLT12Qx5D78"
   },
   "source": [
    "# Entrenamiento directo de HMM con NLTK\n",
    "\n",
    "* clase en python (NLTK) de HMM: https://www.nltk.org/_modules/nltk/tag/hmm.html\n",
    "\n",
    "Ejemplo con el Corpus Treebank en ingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "id": "NVyCuawh5Eqj",
    "outputId": "8f2ef078-4a92-44cd-f614-543ccbe3cff6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /home/oem/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Hacemos el ejemplo con un corpus en ingles, Dataset 'treebank'\n",
    "import nltk\n",
    "nltk.download('treebank')\n",
    "from nltk.corpus import treebank\n",
    "#Como el Dataset ya esta equiquetado tomamos todas las frases\n",
    "#Esconjemos un parte del Dataset para el entrenamiento\n",
    "train_data = treebank.tagged_sents()[:3900]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAHDLyQQcboh"
   },
   "source": [
    "# Carga del modelo HMM previamente entrenado\n",
    "\n",
    "Estructura de la data de entrenamiento. Tener presente que la convención es diferente de la UPOS, ya que el Dataset es antiguo y por ende tiene otra convención. El algoritmo funciona con cualquier convención."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "C_DomEIM5Hif",
    "outputId": "df2a8bd9-2e56-4296-ef9a-c774456ae39e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAHDLyQQcboh"
   },
   "source": [
    "HMM pre-construido en NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "id": "WtknnYIi5KdG",
    "outputId": "e4dbaf5e-7934-4d24-ab4e-e406620c648d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HiddenMarkovModelTagger 46 states and 12385 output symbols>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import hmm\n",
    "tagger = hmm.HiddenMarkovModelTrainer().train_supervised(train_data)\n",
    "tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98
    },
    "id": "tLG-QzKc5OM4",
    "outputId": "c05f89f9-1894-4116-93b2-f2b7d4cf96a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'),\n",
       " ('Vinken', 'NNP'),\n",
       " ('will', 'MD'),\n",
       " ('get', 'VB'),\n",
       " ('old', 'JJ')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag(\"Pierre Vinken will get old\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAHDLyQQcboh"
   },
   "source": [
    "Training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "id": "aGLYRUBb5Wni",
    "outputId": "2ed13ca6-f699-4602-a94f-ef2753f8e461"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9815403947224078"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.evaluate(treebank.tagged_sents()[:3900])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FN-Bgfk-pI0m"
   },
   "source": [
    "## Ejercicio de práctica\n",
    "\n",
    "**Objetivo:** Entrena un HMM usando la clase `hmm.HiddenMarkovModelTrainer()` sobre el dataset `UD_Spanish_AnCora`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrtTL4ihxfiq"
   },
   "source": [
    "1. **Pre-procesamiento:** En el ejemplo anterior usamos el dataset en ingles `treebank`, el cual viene con una estructura diferente a la de `AnCora`, en esta parte escribe código para transformar la estructura de `AnCora` de manera que quede igual al `treebank` que usamos así:\n",
    "\n",
    "$$\\left[ \\left[ (\\text{'El'}, \\text{'DET'}), (\\dots), \\dots\\right], \\left[\\dots \\right] \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "1X8qg5Fc5ahS"
   },
   "outputs": [],
   "source": [
    "# desarrolla tu código aquí \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_OYeCVQrZAK"
   },
   "source": [
    "2. **Entrenamiento:** Una vez que el dataset esta con la estructura correcta, utiliza la clase `hmm.HiddenMarkovModelTrainer()` para entrenar con el $80 \\%$ del dataset como conjunto de `entrenamiento` y $20 \\%$ para el conjunto de `test`.\n",
    "\n",
    "**Ayuda:** Para la separacion entre conjuntos de entrenamiento y test, puedes usar la funcion de Scikit Learn: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "En este punto el curso de Machine Learning con Scikit Learn es un buen complemento para entender mejor las funcionalidades de Scikit Learn: https://platzi.com/cursos/scikitlearn-ml/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "DZpAIB87sTqL"
   },
   "outputs": [],
   "source": [
    "# desarrolla tu código aquí\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLS54wqsu9OK"
   },
   "source": [
    "3. **Validación del modelo:** Un vez entrenado el `tagger`, calcula el rendimiento del modelo (usando `tagger.evaluate()`) para los conjuntos de `entrenamiento` y `test`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "JEwZIG8Du98v"
   },
   "outputs": [],
   "source": [
    "#desarrolla tu código aquí\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "cAHDLyQQcboh",
    "9SNuWx-ScSTg",
    "YjnAshwzxrKZ",
    "BnLT12Qx5D78"
   ],
   "name": "[Lectura_11/12/13]HMM_Viterbi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}