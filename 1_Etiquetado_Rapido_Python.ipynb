{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "1_Etiquetado_Rapido_Python.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bu9C9XvfOl_J"
   },
   "source": [
    "# Etiquetado en NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AlvswJiYTdj"
   },
   "source": [
    "## Pipeline básico para Ingles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBoJtHxBvVLv"
   },
   "source": [
    "## @title Dependencias previas\n",
    "\n",
    "**nltk.download('punkt')** es un `tokenizer`\n",
    "\n",
    "This tokenizer divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences.  It must be trained on a large collection of plaintext in the target language before it can be used. [Punkt](https://www.nltk.org/_modules/nltk/tokenize/punkt.html)\n",
    "\n",
    "**nltk.download('averaged_perceptron_tagger')** es un `tagger`\n",
    "\n",
    "The perceptron part-of-speech tagger implements part-of-speech tagging using the averaged, structured perceptron algorithm. Some information about the implementation is available in this presentation. The implementation is based on the references in the final slide. [Averaged Perceptron Tagger](https://www.nltk.org/_modules/nltk/tag/perceptron.html)\n",
    "\n",
    "Este algoritmo del perceptrón demostro que tiene una eficiencia superior a los etiquetadores por `modelos de máxima entropía` utilizados previamente, que estaban definidos por regresiones logísticas.\n",
    "\n",
    "## Esalera de modelos\n",
    "\n",
    "* **Módelos markovianos latentes (HMM):** Son cadenas de Marcov, que es un conjunto finito de estados, donde se peuden definir estados discretos. Las cadenas de Marcov definen todas las posibles transiciones a traves de probabilidades de transición entre los posibles estados que un sistema puede tener, de lo cual se puede sacar una matriz de transición, tambien hay un vector de estados iniciales, el cual se multiplica con la matriz de transición y el resultado es el siguiente vector de estados iniciales.\n",
    "\n",
    "* **Módelos marcovianos de máxima entropía**\n",
    "* **Deep Learning**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tmb2Cr7v92E7",
    "outputId": "b0b2747c-8954-4155-967e-5644cba95280"
   },
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import word_tokenize"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrXEXChXvVLy"
   },
   "source": [
    "## @title Etiquetado en una línea ...\n",
    "\n",
    "Sobre los tokens extraidos con `word_tokenize` aplico el etiquetado con `nltk.pos_tag(text)`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8lefQX__OtfL",
    "outputId": "3fd4665a-e560-4cb8-f3c1-2ac3d89aa2b9"
   },
   "source": [
    "text = word_tokenize(\"And now here I am enjoying today\")\n",
    "nltk.pos_tag(text)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('here', 'RB'),\n",
       " ('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('enjoying', 'VBG'),\n",
       " ('today', 'NN')]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 2
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2oHdWHBvVL0"
   },
   "source": [
    "## @title Categoria gramatical de cada etiqueta\n",
    "\n",
    "Descargamos los conjuntos de etiquetas con `nltk.download('tagsets')` que nos dan la metadata de lo que significan las etiquetas e imprimimos la metadata"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qocvMilXPdXs",
    "outputId": "d908d15b-b5a5-4273-c2e7-553d48d8dfb9"
   },
   "source": [
    "nltk.download('tagsets')\n",
    "for tag in ['CC', 'RB', 'PRP', 'VBP', 'VBG', 'NN']:\n",
    "  print(nltk.help.upenn_tagset(tag))"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
      "[nltk_data]   Unzipping help/tagsets.zip.\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "None\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "None\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "None\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "None\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "None\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "None\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSXvSMivvVL2"
   },
   "source": [
    "## @title Palabras homónimas\n",
    "\n",
    "Determinamos la etiqueta respectiva a ambigüedades gramaticales en el idioma ingles como el siguiente ejemplo con la palabra `permit`. Al realizar el **pos taging** se asignan sus categoráas gramaticales correctas segñun su uso."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MEo2MWrkPqrO",
    "outputId": "a5ba995d-0067-4050-dd1c-05257049ed41"
   },
   "source": [
    "text = word_tokenize(\"They do not permit other people to get residence permit\")\n",
    "nltk.pos_tag(text)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('They', 'PRP'),\n",
       " ('do', 'VBP'),\n",
       " ('not', 'RB'),\n",
       " ('permit', 'VB'),\n",
       " ('other', 'JJ'),\n",
       " ('people', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('get', 'VB'),\n",
       " ('residence', 'NN'),\n",
       " ('permit', 'NN')]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 4
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIIi46WzYZvc"
   },
   "source": [
    "## Etiquetado en Español \n",
    "\n",
    "Para el ingles, NLTK tiene tokenizador y etiquetador pre-entrenados por defecto. En cambio, para otros idiomas es preciso entrenarlo previamente. \n",
    "\n",
    "* usamos el corpus `cess_esp` https://mailman.uib.no/public/corpora/2007-October/005448.html\n",
    "\n",
    "* el cual usa una convención de etiquetas gramaticales dada por el grupo EAGLES https://www.cs.upc.edu/~nlp/tools/parole-sp.html\n",
    "\n",
    "En el idioma `ingles` los algoritmos ya esta preentrenados, por el contraio para el idioma `español`, no hay algoritmos preentrenados, motivo por el cual debemos entrenar los algoritmos antes de hacer el **pos taging**.\n",
    "\n",
    "Pra trabajar con el español importamos el corpus `cess_esp` e importamos funcionalidades de `NLTK`\n",
    "\n",
    "[UnigramTagger](https://www.kite.com/python/docs/nltk.UnigramTagger)\n",
    "\n",
    "[BigramTagger](https://www.kite.com/python/docs/nltk.BigramTagger)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1uLX4KgDX-cs",
    "outputId": "002fd25e-1250-473c-e50a-c4a95ae325d7"
   },
   "source": [
    "nltk.download('cess_esp')\n",
    "from nltk.corpus import cess_esp as cess\n",
    "from nltk import UnigramTagger as ut\n",
    "from nltk import BigramTagger as bt"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cess_esp to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/cess_esp.zip.\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zB85d-ZVvVL6"
   },
   "source": [
    "## @title Entrenamiendo del tagger por unigramas"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "id": "ObXFbEvRvVL6"
   },
   "source": [
    "#Separamos las frases del corpus\n",
    "cess_sents = cess.tagged_sents() \n",
    "# Obtenemos un fracción del Dataset\n",
    "fraction = int(len(cess_sents)*90/100)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mFi7BLojYw-F",
    "pycharm": {
     "is_executing": true
    },
    "outputId": "591cc1b2-891a-4a71-f224-dcf9ffae031d"
   },
   "source": [
    "# Definimos una instancia del etiquetador por unigramas\n",
    "# Le pasamos una francción del Dataset para realizar el entrenamiento\n",
    "# Lo entrenamos con el 90% del conjunto de datos\n",
    "uni_tagger = ut(cess_sents[:fraction]) \n",
    "# Despues de entrenar hacemos la evaluación con el resto del Dataset\n",
    "uni_tagger.evaluate(cess_sents[fraction+1:])\n",
    "\n",
    "# Al final se obtiene la métrica de la asignación de etiquetas"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8069484240687679"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 8
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1Gr3cfOvVL7"
   },
   "source": [
    "Aplicamos el algoritmo previamente entrenado y evaluado"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HHA32_xVg81K",
    "pycharm": {
     "is_executing": true
    },
    "outputId": "93ed91ee-fbb3-4d23-83ed-6622cac41b29"
   },
   "source": [
    "uni_tagger.tag(\"Yo soy una persona muy amable\".split(\" \"))"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Yo', 'pp1csn00'),\n",
       " ('soy', 'vsip1s0'),\n",
       " ('una', 'di0fs0'),\n",
       " ('persona', 'ncfs000'),\n",
       " ('muy', 'rg'),\n",
       " ('amable', None)]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 9
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "LHrhAnqYvVL8"
   },
   "source": [
    "## @title Entrenamiento del tagger por bigramas"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EbE3V0hdvVL9",
    "outputId": "78b600ad-05ea-4837-c523-c541e7def67e"
   },
   "source": [
    "# Obtenemos un fracción del Dataset\n",
    "fraction = int(len(cess_sents)*90/100)\n",
    "# Definimos una instancia del etiquetador por bigramas\n",
    "# Le pasamos una francción del Dataset para realizar el entrenamiento\n",
    "# Lo entrenamos con el 90% del conjunto de datos\n",
    "bi_tagger = bt(cess_sents[:fraction])\n",
    "# Despues de entrenar hacemos la evaluación con el resto del Dataset\n",
    "bi_tagger.evaluate(cess_sents[fraction+1:])\n",
    "\n",
    "# Al final se obtiene la métrica de la asignación de etiquetas"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.1095272206303725"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 10
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Co7t-oVFvVL9",
    "outputId": "56e2824f-7478-45af-b655-11cdc56e1b2b"
   },
   "source": [
    "bi_tagger.tag(\"Yo soy una persona muy amable\".split(\" \"))"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Yo', 'pp1csn00'),\n",
       " ('soy', 'vsip1s0'),\n",
       " ('una', None),\n",
       " ('persona', None),\n",
       " ('muy', None),\n",
       " ('amable', None)]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 11
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFsEmxLFvVL-"
   },
   "source": [
    "<font color=\"green\"> Al analizar las pruebas previamente descritas podemos decir que el etiquetador por `unigramas` es mejor que el etiquedador por `bigramas` y no se recomienda usar el segundo </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m50bRKLlikbx"
   },
   "source": [
    "# Etiquetado mejorado con Stanza (StanfordNLP)\n",
    "\n",
    "**¿Que es Stanza?**\n",
    "\n",
    "* El grupo de investigacion en NLP de Stanford tenía una suite de librerias que ejecutaban varias tareas de NLP, esta suite se unifico en un solo servicio que llamaron **CoreNLP** con base en codigo java: https://stanfordnlp.github.io/CoreNLP/index.html\n",
    "\n",
    "* Para python existe **StanfordNLP**: https://stanfordnlp.github.io/stanfordnlp/index.html\n",
    "\n",
    "* Sin embargo, **StanfordNLP** ha sido deprecado y las nuevas versiones de la suite de NLP reciben mantenimiento bajo el nombre de **Stanza**: https://stanfordnlp.github.io/stanza/"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WY_YTM6TirMd",
    "pycharm": {
     "is_executing": true
    },
    "outputId": "ea4698c5-3004-4f76-a2e9-10bb5fd1efdf"
   },
   "source": [
    "!pip install stanza"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Collecting stanza\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/e7/8b/3a9e7a8d8cb14ad6afffc3983b7a7322a3a24d94ebc978a70746fcffc085/stanza-1.1.1-py3-none-any.whl (227kB)\n",
      "\r\u001B[K     |█▍                              | 10kB 17.1MB/s eta 0:00:01\r\u001B[K     |██▉                             | 20kB 22.2MB/s eta 0:00:01\r\u001B[K     |████▎                           | 30kB 10.9MB/s eta 0:00:01\r\u001B[K     |█████▊                          | 40kB 8.9MB/s eta 0:00:01\r\u001B[K     |███████▏                        | 51kB 4.3MB/s eta 0:00:01\r\u001B[K     |████████▋                       | 61kB 4.8MB/s eta 0:00:01\r\u001B[K     |██████████                      | 71kB 5.0MB/s eta 0:00:01\r\u001B[K     |███████████▌                    | 81kB 5.5MB/s eta 0:00:01\r\u001B[K     |█████████████                   | 92kB 5.5MB/s eta 0:00:01\r\u001B[K     |██████████████▍                 | 102kB 4.2MB/s eta 0:00:01\r\u001B[K     |███████████████▉                | 112kB 4.2MB/s eta 0:00:01\r\u001B[K     |█████████████████▎              | 122kB 4.2MB/s eta 0:00:01\r\u001B[K     |██████████████████▊             | 133kB 4.2MB/s eta 0:00:01\r\u001B[K     |████████████████████▏           | 143kB 4.2MB/s eta 0:00:01\r\u001B[K     |█████████████████████▋          | 153kB 4.2MB/s eta 0:00:01\r\u001B[K     |███████████████████████         | 163kB 4.2MB/s eta 0:00:01\r\u001B[K     |████████████████████████▌       | 174kB 4.2MB/s eta 0:00:01\r\u001B[K     |██████████████████████████      | 184kB 4.2MB/s eta 0:00:01\r\u001B[K     |███████████████████████████▍    | 194kB 4.2MB/s eta 0:00:01\r\u001B[K     |████████████████████████████▉   | 204kB 4.2MB/s eta 0:00:01\r\u001B[K     |██████████████████████████████▎ | 215kB 4.2MB/s eta 0:00:01\r\u001B[K     |███████████████████████████████▊| 225kB 4.2MB/s eta 0:00:01\r\u001B[K     |████████████████████████████████| 235kB 4.2MB/s \n",
      "\u001B[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanza) (3.12.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanza) (1.19.4)\n",
      "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from stanza) (1.7.0+cu101)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanza) (2.23.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanza) (4.41.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (51.0.0)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (3.7.4.3)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.16.0)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (1.24.3)\n",
      "Installing collected packages: stanza\n",
      "Successfully installed stanza-1.1.1\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRMqFUm8vVMF"
   },
   "source": [
    "## Esta parte puede demorar un poco ...."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HG-Sdn6QmHgR",
    "pycharm": {
     "is_executing": true
    },
    "outputId": "1839be94-04f1-428a-fc0c-4fe61ad5dce6"
   },
   "source": [
    "import stanza\n",
    "stanza.download('es')"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 22.2MB/s]                    \n",
      "2020-12-28 00:28:33 INFO: Downloading default packages for language: es (Spanish)...\n",
      "Downloading http://nlp.stanford.edu/software/stanza/1.1.0/es/default.zip: 100%|██████████| 583M/583M [03:56<00:00, 2.47MB/s]\n",
      "2020-12-28 00:32:39 INFO: Finished downloading models and saved to /root/stanza_resources.\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnDw440Aw_lP"
   },
   "source": [
    "Stanza funciona con `piplines`, lo que significa pegar distintas tareas del lenguaje natural, una tras otra. Dentro del pipline definirmos las estapas, para este caso son dos etapas `tokenize,pos`,las cuales estan entrenadas con el paquete `ancora`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B2sKNXawmWH7",
    "pycharm": {
     "is_executing": true
    },
    "outputId": "380b7a40-89b6-4d90-f07c-f3d8715394fc"
   },
   "source": [
    "nlp = stanza.Pipeline('es', processors='tokenize,pos')\n",
    "doc = nlp('yo soy una persona muy amable')"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2020-12-28 00:36:53 INFO: Loading these models for language: es (Spanish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "| pos       | ancora  |\n",
      "=======================\n",
      "\n",
      "2020-12-28 00:36:53 INFO: Use device: cpu\n",
      "2020-12-28 00:36:53 INFO: Loading: tokenize\n",
      "2020-12-28 00:36:53 INFO: Loading: pos\n",
      "2020-12-28 00:36:54 INFO: Done loading processors!\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4pYpAldxwoa"
   },
   "source": [
    "Recorremos las sentencias dentro de la variable `doc`, las cuales a su vez tiene varias palabras. Las recorremos con base a esta definición y las mostramos. **Stanza** utiliza una convesión de etiquetado familiar para comunidad de etiquetado del lenguaje natural moderno."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86rL3IJRm1oI",
    "pycharm": {
     "is_executing": true
    },
    "outputId": "5b52614d-8566-4774-e270-c92165c05464"
   },
   "source": [
    "for sentence in doc.sentences:\n",
    "  for word in sentence.words:\n",
    "    print(word.text, word.pos)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "yo PRON\n",
      "soy AUX\n",
      "una DET\n",
      "persona NOUN\n",
      "muy ADV\n",
      "amable ADJ\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsZF93NQodxX"
   },
   "source": [
    "# Referencias adicionales:\n",
    "\n",
    "* Etiquetado POS con Stanza https://stanfordnlp.github.io/stanza/pos.html#accessing-pos-and-morphological-feature-for-word\n",
    "\n",
    "* Stanza | Github: https://github.com/stanfordnlp/stanza\n",
    "\n",
    "* Articulo en ArXiv: https://arxiv.org/pdf/2003.07082.pdf"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "a2gkMtDvyK_F",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}