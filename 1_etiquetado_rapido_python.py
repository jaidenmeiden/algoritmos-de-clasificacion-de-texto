# -*- coding: utf-8 -*-
"""1_Etiquetado_Rapido_Python.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jB4SBcEGtk0yHLgrB43ecOjqKs_5_3Z7

# Etiquetado en NLTK

## Pipeline básico para Ingles

## @title Dependencias previas
"""

import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
from nltk import word_tokenize

"""## @title Etiquetado en una línea ...

Sobre los tokens extraidos con `word_tokenize` aplico el etiquetado con `nltk.pos_tag(text)`
"""

text = word_tokenize("And now here I am enjoying today")
nltk.pos_tag(text)

"""## @title Categoria gramatical de cada etiqueta

Descargamos los conjuntos de etiquetas con `nltk.download('tagsets')` que nos dan la metadata de lo que significan las etiquetas e imprimimos la metadata
"""

nltk.download('tagsets')
for tag in ['CC', 'RB', 'PRP', 'VBP', 'VBG', 'NN']:
  print(nltk.help.upenn_tagset(tag))

"""## @title Palabras homónimas

Determinamos la etiqueta respectiva a ambigüedades gramaticales en el idioma ingles como el siguiente ejemplo con la palabra `permit`. Al realizar el **pos taging** se asignan sus categoráas gramaticales correctas segñun su uso.
"""

text = word_tokenize("They do not permit other people to get residence permit")
nltk.pos_tag(text)

"""## Etiquetado en Español 

Para el ingles, NLTK tiene tokenizador y etiquetador pre-entrenados por defecto. En cambio, para otros idiomas es preciso entrenarlo previamente. 

* usamos el corpus `cess_esp` https://mailman.uib.no/public/corpora/2007-October/005448.html

* el cual usa una convención de etiquetas gramaticales dada por el grupo EAGLES https://www.cs.upc.edu/~nlp/tools/parole-sp.html

En el idioma `ingles` los algoritmos ya esta preentrenados, por el contraio para el idioma `español`, no hay algoritmos preentrenados, motivo por el cual debemos entrenar los algoritmos antes de hacer el **pos taging**.

Pra trabajar con el español importamos el corpus `cess_esp` e importamos funcionalidades de `NLTK`

[UnigramTagger](https://www.kite.com/python/docs/nltk.UnigramTagger)

[BigramTagger](https://www.kite.com/python/docs/nltk.BigramTagger)
"""

nltk.download('cess_esp')
from nltk.corpus import cess_esp as cess
from nltk import UnigramTagger as ut
from nltk import BigramTagger as bt

"""## @title Entrenamiendo del tagger por unigramas"""

#Separamos las frases del corpus
cess_sents = cess.tagged_sents() 
# Obtenemos un fracción del Dataset
fraction = int(len(cess_sents)*90/100)

# Definimos una instancia del etiquetador por unigramas
# Le pasamos una francción del Dataset para realizar el entrenamiento
# Lo entrenamos con el 90% del conjunto de datos
uni_tagger = ut(cess_sents[:fraction]) 
# Despues de entrenar hacemos la evaluación con el resto del Dataset
uni_tagger.evaluate(cess_sents[fraction+1:])

# Al final se obtiene la métrica de la asignación de etiquetas

"""Aplicamos el algoritmo previamente entrenado y evaluado"""

uni_tagger.tag("Yo soy una persona muy amable".split(" "))

"""## @title Entrenamiento del tagger por bigramas"""

# Obtenemos un fracción del Dataset
fraction = int(len(cess_sents)*90/100)
# Definimos una instancia del etiquetador por bigramas
# Le pasamos una francción del Dataset para realizar el entrenamiento
# Lo entrenamos con el 90% del conjunto de datos
bi_tagger = bt(cess_sents[:fraction])
# Despues de entrenar hacemos la evaluación con el resto del Dataset
bi_tagger.evaluate(cess_sents[fraction+1:])

# Al final se obtiene la métrica de la asignación de etiquetas

bi_tagger.tag("Yo soy una persona muy amable".split(" "))

"""<font color="green"> Al analizar las pruebas previamente descritas podemos decir que el etiquetador por `unigramas` es mejor que el etiquedador por `bigramas` y no se recomienda usar el segundo </font>

# Etiquetado mejorado con Stanza (StanfordNLP)

**¿Que es Stanza?**

* El grupo de investigacion en NLP de Stanford tenía una suite de librerias que ejecutaban varias tareas de NLP, esta suite se unifico en un solo servicio que llamaron **CoreNLP** con base en codigo java: https://stanfordnlp.github.io/CoreNLP/index.html

* Para python existe **StanfordNLP**: https://stanfordnlp.github.io/stanfordnlp/index.html

* Sin embargo, **StanfordNLP** ha sido deprecado y las nuevas versiones de la suite de NLP reciben mantenimiento bajo el nombre de **Stanza**: https://stanfordnlp.github.io/stanza/
"""

!pip install stanza

"""## Esta parte puede demorar un poco ...."""

import stanza
stanza.download('es')

"""Stanza funciona con `piplines`, lo que significa pegar distintas tareas del lenguaje natural, una tras otra. Dentro del pipline definirmos las estapas, para este caso son dos etapas `tokenize,pos`,las cuales estan entrenadas con el paquete `ancora`"""

nlp = stanza.Pipeline('es', processors='tokenize,pos')
doc = nlp('yo soy una persona muy amable')

"""Recorremos las sentencias dentro de la variable `doc`, las cuales a su vez tiene varias palabras. Las recorremos con base a esta definición y las mostramos. **Stanza** utiliza una convesión de etiquetado familiar para comunidad de etiquetado del lenguaje natural moderno."""

for sentence in doc.sentences:
  for word in sentence.words:
    print(word.text, word.pos)

"""# Referencias adicionales:

* Etiquetado POS con Stanza https://stanfordnlp.github.io/stanza/pos.html#accessing-pos-and-morphological-feature-for-word

* Stanza | Github: https://github.com/stanfordnlp/stanza

* Articulo en ArXiv: https://arxiv.org/pdf/2003.07082.pdf
"""

